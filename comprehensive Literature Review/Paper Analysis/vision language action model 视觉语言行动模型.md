vision language action model是能根据用户的话和周围环境的视频输入，输出机器行动指令的多模态模型。这轮具身智能突然爆火最大原因就是LLM在具身智能领域的运用克服了以往基于强化学习和模仿学习训练慢、环境适应能力差、跨任务难迁移的问题。LLM在具身领域的运用最突出的表现就是LVM。LVA的概念提出是在Google Deepmind团队的工作RT2，这是一个系列的工作，包括了RT1、RT2、RT-X、RT-H。目前LVA在具身感知上还很弱，具体原因我认为有两点，一方面是数据，LLM能短时间爆发是依托于互联网过去三十年的语料积累，而具身感知数据并不是常规数据，需要专用采集设备。目前主要的具身感知数据是在仿真环境下生成的。破局方法下个帖子说。LVA弱的另一个原因我认为是模型结构，现在的LVA模型延伸至LLM和LVM，而LLM的结构是适配1D数据的，LVM的结构是适配2D数据的，LVA需要适配3D数据，并且要能适应真实世界，真实世界是可以交互的（具有可供性）。这显然不同于LLM和LVM。此外现有LLM和LVM都是单序列输入和输出的，无法并行。具身智能可能需要同时输入视频、音频、触觉等模态信息，同时输出动作、语音信息。LVA提出于23年，未来的发展潜力巨大，是AGI实现的重要路径。